---
title: "Paper Summary: A Few Useful Things to Know About Machine Learning"
author: "Jaeyoon Han"
date: '2016-12-27'
output: html_document
layout: post
image: /assets/images/papers.jpg
categories: machine-learning
---





## A Few Useful Things to Know About Machine Learning (Domingos, P., 2012)

---

#### 1. Introduction

-	In this paper, the author focused on classification.

---

#### 2. Learning = Representation + Evaluation + Optimization

-	The components for choosing ML algorithm to use:
	-	**Representation.**
		-	Choosing a representation for a learner is tantamount to choosing the set of classifiers that it can possibly learn.
		-	This set is called the *hypothesis space* of the learner.
			-	If a classifier is not in the hypothesis space, it cannot be learned.
	-	**Evaluation.**
		-	An evaluation function (also called *objective function* or *scoring function*) is needed to distinguish good classifiers from bad ones.
	-	**Optimization.**
		-	The choice of optimization technique is key to the efficiency of the learner, and also helps determine the classifier produced if the evaluation function has more than one optimum.
		-	It is common for new learners to start out using off-the-shelf optimizers, which are later replaced by custom-designed ones.

<figure>
  <center>
  <img src="/assets/article_images/2016-12-27-domingos2012/table1.png" width="600px">
  <figcaption>Table 1: The three components of learning algorithms.</figcaption>
</center>
</figure>

-	Most textbooks are organized by representation, and it's easy to overlook the fact that the other components are equally important.

---

#### 3. It's Generalization That Counts

-	The fundamental goal of machine learning is to *generalize* beyond the examples in the training set.
	-	No matter how much data we have, it is very unlikely that we will see those exact examples again at test time.
	-	The most common mistake among machine learning beginners is to test on the training data and have the illusion of success.
		-	If you hire someone to build a classifier, be sure to keep some of the data to yourself and test the classifier they give you on it.
		-	Contamination of your classifier by test data can occur in insidious ways. <br> (Use test data to tune parameters and do a lot of tuning)
	-	Do cross-validation to mitigate some problems.
	-	It is inevitable that using training error as a surrogate for test error, this is fraught with danger.
	-	Optimization problems
		-	We don't have access to the function we want to optimize.
		-	Since the objective function is only a proxy for the true goal, we may not need to fully optimize it.

---

#### 4. Data Alone is Not Enough

-	Generalization being the goal has another major consequence: data alone is not enough, no matter how much of it you have.
-	**No Free Lunch** Theorem
	-	According to which no learner can beat random guessing over all possible functions to be learned.
-	Why machine learning has been so successful
	-	The functions we want to learn in the real world are *not* drawn uniformly from the set of all mathematically possible functions.
	-	Induction is a vastly more powerful lever than deduction, requiring much less input knowledge to produce useful results, but it still needs more than zero input knowledge to work.
-	One of the key criteria for choosing a representation is which kinds of knowledge are easily expressed in it.
	-	The most useful learners in this regard are those that don't just have assumptions hard-wired into them, but allow us to state them explicitly, vary them widely, and incorporate them automatically into the learning.

---

#### 5. Overfitting Has Many Faces

<figure>
  <center>
  <img src="/assets/article_images/2016-12-27-domingos2012/figure1.png" width="400px">
  <figcaption>Figure 1. Bias and variance in dart-throwing.</figcaption>
</center>
</figure>

-	Overfitting

	-	The bugbear of machine learning
	-	When my learner outputs a classifier that is 100% accurate on the training data but only 50% accurate on test data, when in fact it could have output one that is 75% accurate on both, it has overfit.

-	Way to understand overfitting: **Bias-Variance Decomposition**

	-	Bias: A learner's tendency to consistently learn the same wrong things.
	-	Variance: The tendency to learn random things irrespective of the real signal.

-	A linear learner has high bias: because when the frontier between two classes is not a hyperplane the learner is unable to induce it.

-	Decision trees don't have this problem because they can represent any Boolean variance.

	-	But they can suffer from high variance!
	-	Decision trees learned on different training sets generated by the same phenomenon are often very different, when in fact they should be the same.

<figure>
  <center>
  <img src="/assets/article_images/2016-12-27-domingos2012/figure2.png" width="400px">
  <figcaption>Figure 2. Naive Bayes can outperform a state-of-the-art rule learner (C4.5 rules) even when the true classifier is a set of rules.</figcaption>
</center>
</figure>

-	Figure 2 illustrates that even though the true classifier is a set of rules, with up to 1000 examples naive Bayes is more accurate than a rule learner.
-	Strong false assumptions can be better than weak true ones, because a learner with the latter needs more data to avoid overfitting.
-	Cross-validation and adding a **regularization term** can help to combat overfitting.
	-	Adding a regularization term to the evaluation function can, for example, penalize classifiers with more structure, thereby favoring smaller ones with less room to overfit.

- A common misconception about overfitting is that it is caused by noise. This can be aggravate overfitting, by making the learner draw a capricious frontier. But severe overfitting can occur even in the absence of noise.

---

#### 6. Intuition Fails In High Dimensions

- **The curse of dimensionality**
	- As the number of feature (dimensionality) grows, the amount of data would need to fit a reasonable model grows exponentially.
	- The similarity-based reasoning that machine learning algorithms depend on breaks down in high dimensions.
	- Our institutions, which come from a three-dimensional world, often do not apply in high-dimensional ones.
	- Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact their benefits may be outweighted by the curse of dimensionality.

- **The blessing of non-uniformity**
	- In most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near a lower-dimensional manifold.
		- kNN works quite well for handwritten digit recognition even though images of digits have one dimension per pixel, because the space of digit images is much smaller than the space of all possible images.
	- Learners can implicitly take advantage of this lower effective dimension, or algorithms for explicitly reducing the dimensionality can be used.

---

#### 7. Theoretical Guarantees Are Not What They Seem

- The main role of theoretical guarantees in machine learning is not as a criterion for practical decisions, but as a source of understanding and driving force for algorithm design.
	- The most common type of theoretical guarantees is a bound on the number of examples needed to ensure good generalization.
	- Another common type is asymptotic: given infinite data, the learner is guaranteed to output the correct classifier.
		- Note that because of the bias-variance trade-off, if learner A is better than learner B given infinite data, B is often better than A given finite data.

---

#### 8. Feature Engineering Is The Key

- The most important factor which makes the difference is the features used.
	- But, the raw data is not in a form that is amenable to learning, but you can construct features from it that are.
- Always, machine learning is not a one-shot process of building a data set and running a learner, but rater an iterative process of running the learner, analyzing the results, modifying the data and/or the learner, and repeating.
	- Learning is easy and the quickest part, but feature engineering is more difficult because it's domain-specific.

- One of the holy grails of machine learning is to automate more and more of the feature engineering process.
	- In nowadays, it is by automatically generating large number of candidate features and selecting the best by their information gain with respect to the class.
	- Note that features that look irrelevant in isolation may be relevant in combination.

---

#### 9. More Data Beats A Cleverer Algorithm

- Design a better learning algorithm / Gather more data
	- Pragmatically, the quickest path to success is often to just get more data.
	- But, it can bring up another problem: scalability.

- Part of the reason using cleverer algorithm has a smaller payoff than you might expect is that, to a first approximation, they all do the same.

<figure>
  <center>
  <img src="/assets/article_images/2016-12-27-domingos2012/figure3.png" width="400px">
  <figcaption>Figure 3. Very different frontiers can yield similar class predictions. (+ and - are training examples of two classes.)</figcaption>
</center>
</figure>

- More complex learners are seductive, but they are usually harder to use.
- Learners can be divided into two major types:
	- Those whose representation has fixed size: **Parametric learners** (e.g. Linear regression)
		- It can only take advantage of so much data.
	- Those whose representation can grow with the data: **Non-parametric learners** (e.g. Decision tree)
		- It can in principle learn any function given sufficient data, but in practice they may not, because of limitations of the algorithm or computational cost.
		- Because of the curse of dimensionality, no existing amount of data may be enough.

---

#### 10. Learn Many Models, Not Just One

- In the early days, most effort went into trying many variations of it and selecting the best one.
- Nowadays, still trying many variations of many learners and selecting just the best one. But researchers combine many variations instead of selecting the best one.
	- Creating such **model ensembles** is now standard.
		- *Bagging*, *Boosting*, *Stacking*
- Model ensembles should not be confused with Bayesian model averaging(BMA).
	- BMA
		- the theoretically optimal approach to learning.
		- Predictions on new examples are made by averaging the individual predictions of all classifiers in the hypothesis space, weighted by how ell the classifiers explain the training data and how much we believe in them *a priori*.
- BMA and ensembles are very different.
	- Ensembles : Change the hypothesis space, and can take a wide variety of forms.
	- BMA : It assigns weights to the hypotheses in the original space according to a fixed formula.

---

#### 11. Simplicity Does Not Imply Accuracy

- Occam's razor can be applied in machine learning, but there are many counterexamples to it, and the "no free lunch" theorems imply it cannot be true.
	- Counterexamples: Ensembles learning
- A more sophisticated view: smaller hypothesis spaces allow hypotheses to be represented by shorter codes.
- A further complication arises from the fact that few learners search their hypothesis space exhaustively.
- Simpler hypotheses should be preferred because simplicity is a virtue in its own right, not because of a hypothetical connection with accuracy.

---

#### 12. Representable Does Not Imply Learnable

---

#### 13. Correlation Does Not Imply Causation

- The point that correlation does not imply causation is made so often that it is perhaps not worth belaboring.
- Machine learning is usually applied to *observational* data, where the predictive variables are not under the control of the learner, as opposed to *experimental* data, where they are.
- Many researchers believe that causality is only a convenient fiction.
