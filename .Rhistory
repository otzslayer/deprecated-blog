confusionMatrix(knn, factor(test$Survived), positive = "1")
doMC::registerDoMC(4)
set.seed(7)
linear.svm.cv <- tune.svm(x = train_mat, y = factor(train$Survived), kernel = "linear", cost = c(0.001, 0.01, 0.1, 1, 5, 10),
tunecontrol = tune.control(sampling = "cross"))
linear.svm.cv
linear.svm <- predict(linear.svm.cv$best.model, test_mat)
confusionMatrix(linear.svm, test$Survived, positive = "1")
set.seed(7)
poly.svm.cv <- tune.svm(x = train_mat, y = factor(train$Survived), kernel = "polynomial",
degree = c(2, 3, 4), coef0 = c(0.1, 0.5, 1, 2),
cost = c(0.001, 0.01, 0.1, 1, 3, 5),
tunecontrol = tune.control(sampling = "cross"))
poly.svm.cv
poly.svm.cv$best.parameters
linear.svm.cv
linear.svm.cv$best.parameters
linear.svm.cv
linear.svm.cv$best.parameters
poly.svm.cv$best.parameters
linear.svm.cv$best.parameters[1,1]
linear.svm.cv$best.parameters[1,"cost"]
library(knitr)
library(rmdformats)
library(ggplot2)
library(MASS)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, prompt = FALSE,
tidy = FALSE, comment = NA, warning = FALSE, cache = TRUE,
fig.height = 5, fig.width = 8, fig.retina = 2,
fig.align = "center")
## Global options
custom_theme <- theme_bw(base_family = "Open Sans") +
theme(axis.title.x = element_text(size = 11,
margin = margin(10, 0, 0, 0),
face = "bold"),
axis.title.y = element_text(size = 11,
margin = margin(0, 10, 0, 0),
face = "bold"),
plot.title = element_text(family = "Open Sans", face = "bold", size = 20),
panel.background = element_blank(),
axis.text.x = element_text(angle = 0, face = "italic", vjust = .1),
axis.text.y = element_text(face = "italic"), legend.position = "bottom",
legend.title = element_text(size = 9, face = 'bold.italic'))
theme_set(custom_theme)
library(readr)
library(dplyr)
library(ggplot2)
titanic <- read_csv("titanic.csv")
head(titanic)
titanic <- titanic %>%
mutate(ID = 1:nrow(titanic)) %>%
select(ID, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked, survived)
names(titanic) <- c("ID", "Pclass", "Name", "Sex", "Age", "SibSp", "Parch", "Ticket", "Fare", "Cabin", "Embarked", "Survived")
head(titanic)
sum(is.na(titanic$Embarked))        # 2 NA
titanic[which(is.na(titanic$Embarked)), ]
titanic[!is.na(titanic$Embarked), ] %>%
ggplot(aes(x = Embarked, y = Fare, fill = factor(Pclass))) +
geom_boxplot() +
geom_hline(aes(yintercept = 80), colour = "red")
titanic[which(is.na(titanic$Embarked)), "Embarked"] <- "C"
titanic[which(is.na(titanic$Fare)), ]
titanic %>%
group_by(Pclass, Embarked) %>%
summarise(median(Fare, na.rm = T))
titanic[which(is.na(titanic$Fare)), "Fare"] <- 8.05
splitToTitle <- function(x){
strsplit(x, split = '[.,]')[[1]][2]
}
titanic$Title <- sapply(titanic$Name, FUN = splitToTitle)
titanic$Title <- sub(' ', '', titanic$Title)        # 빈공간 제거
table(titanic$Title)
# Mlle = Mademoiselle = Miss, Mme = Madame = Mrs
titanic$Title[titanic$Title %in% c("Capt", "Major", "Col", "Dr", "Rev")] <- "Officer"
titanic$Title[titanic$Title %in% c("Jonkheer", "Don", "Sir", "the Countess", "Dona", "Lady")] <- "Royalty"
titanic$Title[titanic$Title %in% c("Mme", "Ms", "Mrs")] <- "Mrs"
titanic$Title[titanic$Title %in% c("Mlle", "Miss")] <- "Miss"
titanic$Title <- as.factor(titanic$Title)
table(titanic$Title)
titanic <- titanic %>%
mutate(FamilySize = Parch + SibSp + 1) %>%
mutate(FamilyType = ifelse(FamilySize == 1, "Singletone",
ifelse(FamilySize >= 2 & FamilySize < 5, "SmallFamily",
ifelse(FamilySize >= 5, "LargeFamily", 0))))
head(titanic)
AgeTable <- titanic %>%
group_by(Sex, Pclass, Title) %>%
summarise(Age = round(mean(Age, na.rm = TRUE)))
AgeTable <- data.frame(AgeTable)
titanic %>%
filter(!is.na(Age)) %>%
group_by(Sex, Pclass, Title) %>%
ggplot(aes(x = Title, y = Age, fill = factor(Pclass))) +
geom_boxplot() +
facet_wrap(~Sex, nrow = 2)
for(i in 1:nrow(AgeTable)){
titanic[is.na(titanic$Age) & titanic$Sex == AgeTable[i, 1] & titanic$Pclass == AgeTable[i, 2] & titanic$Title == AgeTable[i, 3], "Age"] <- AgeTable[i, 4]
}
titanic$Age <- ceiling(titanic$Age)
Cabin <- titanic$Cabin
extractCabin <- function(x){
strsplit(x, split = ' ')[[1]][1]
}
Cabin <- sapply(Cabin, extractCabin)
names(Cabin) <- NULL
Cabin <- gsub("[[:digit:]]", "", Cabin)
Cabin[is.na(Cabin)] <- "U"
Cabin[which(Cabin == "T")] <- "A"
titanic$Cabin <- Cabin
titanic <- titanic %>%
mutate(Child = ifelse(Age < 18, 1, 0))
titanic <- titanic %>%
mutate(Mother = ifelse(Sex == "female" & Parch > 0 & Age > 18 & Title != "Miss", 1, 0))
head(titanic)
set.seed(7)
trainIndex <- sample(1:nrow(titanic), 1000)
train <- titanic[trainIndex, ] %>%
select(-c(ID, Name, Ticket))
test <- titanic[-trainIndex, ] %>%
select(-c(ID, Name, Ticket))
train$Survived <- factor(train$Survived)
test$Survived <- factor(test$Survived)
library(Matrix)
train_mat <- model.matrix(data = train, Survived ~ .)[, -1]
test_mat <- model.matrix(data = test, Survived ~ .)[, -1]
library(Boruta)
set.seed(7)
bor.result <- Boruta(train_mat, train$Survived, doTrace = 1)
cat("\nSelected Features")
getSelectedAttributes(bor.result)
plot(bor.result)
bor.result$finalDecision
train_mat <- train_mat[, getSelectedAttributes(bor.result)]
test_mat <- test_mat[, getSelectedAttributes(bor.result)]
library(e1071)
set.seed(7)
knn.cv <- tune.knn(x = train_mat, y = factor(train$Survived), k = seq(1, 40, by = 2),
tunecontrol = tune.control(sampling = "cross"), cross = 10)
knn.cv
plot(knn.cv)
library(class)
knn <- knn(train_mat, test_mat, factor(train$Survived), k = knn.cv$best.parameters[, 1])
library(caret)
confusionMatrix(knn, factor(test$Survived), positive = "1")
doMC::registerDoMC(4)
set.seed(7)
linear.svm.cv <- tune.svm(x = train_mat, y = factor(train$Survived), kernel = "linear", cost = c(0.001, 0.01, 0.1, 1, 5, 10),
tunecontrol = tune.control(sampling = "cross"))
linear.svm.cv
linear.svm <- predict(linear.svm.cv$best.model, test_mat)
confusionMatrix(linear.svm, test$Survived, positive = "1")
set.seed(7)
poly.svm.cv <- tune.svm(x = train_mat, y = factor(train$Survived), kernel = "polynomial",
degree = c(2, 3, 4), coef0 = c(0.1, 0.5, 1, 2),
cost = c(0.001, 0.01, 0.1, 1, 3, 5),
tunecontrol = tune.control(sampling = "cross"))
poly.svm.cv
poly.svm <- predict(poly.svm.cv$best.model, test_mat)
confusionMatrix(poly.svm, test$Survived, positive = "1")
set.seed(7)
radial.svm.cv <- tune.svm(x = train_mat, y = factor(train$Survived), kernel = "radial",
gamma = c(0.1, 0.5, 1, 2, 3), coef0 = c(0.1, 0.5, 1, 2),
cost = c(0.001, 0.01, 0.1, 1, 3, 5),
tunecontrol = tune.control(sampling = "cross"))
radial.svm.cv
radial.svm <- predict(radial.svm.cv$best.model, test_mat)
confusionMatrix(radial.svm, test$Survived, positive = "1")
library(ranger)
set.seed(7)
rf <- ranger(Survived ~ ., data = train, num.trees = 2000)
randomForest <- predict(rf$forest, test)
randomForest <- randomForest$predictions
confusionMatrix(randomForest, test$Survived, positive = "1")
library(glmnet)
set.seed(7)
logit_L1.cv <- cv.glmnet(x = train_mat, y = train$Survived, family = 'binomial',
alpha = 1)
logit.L1 <- predict(logit_L1.cv, test_mat, s = logit_L1.cv$lambda.min, type = 'response')
logit.L1 <- as.vector(ifelse(logit.L1 > 0.5, 1, 0))
confusionMatrix(logit.L1, test$Survived, positive = "1")
set.seed(7)
logit_L2.cv <- cv.glmnet(x = train_mat, y = train$Survived, family = 'binomial',
alpha = 0)
logit.L2 <- predict(logit_L2.cv, test_mat, s = logit_L2.cv$lambda.min, type = 'response')
logit.L2 <- as.vector(ifelse(logit.L2 > 0.5, 1, 0))
confusionMatrix(logit.L2, test$Survived, positive = "1")
library(caret)
library(xgboost)
trCtrl <- trainControl(method = "repeatedcv", number = 10, allowParallel = TRUE)
xgb.grid <- expand.grid(nrounds = c(180, 200, 220),
eta = c(0.01, 0.03, 0.05),
max_depth = c(3, 5, 7),
gamma = 0,
colsample_bytree = c(0.6, 0.8, 1),
min_child_weight = c(0.8, 0.9, 1),
subsample = 1
)
set.seed(7)
doMC::registerDoMC(4)
xgbTrain <- train(x = train_mat,
y = train$Survived,
objective = "binary:logistic",
trControl = trCtrl,
tuneGrid = xgb.grid,
method = "xgbTree"
)
xgbTrain$bestTune
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
folded_train <- train %>%
mutate(foldID = rep(1:5, each = nrow(train)/5)) %>%
select(foldID, everything())
folded_mat <- model.matrix(data = folded_train, Survived ~ .)[, -1]
### Initiating ###
k_NN <- NULL
svm.linear <- NULL
svm.poly <- NULL
svm.radial <- NULL
rf <- NULL
logitL1 <- NULL
logitL2 <- NULL
XGB <- NULL
for(targetFold in 1:5){
trainFold <- filter(folded_train, foldID != targetFold) %>%
select(-foldID)
trainSurvived <- trainFold$Survived
testFold <- filter(folded_train, foldID == targetFold) %>%
select(-foldID)
testSurvived <- testFold$Survived
fold_train_mat <- folded_mat[folded_mat[, 1] != targetFold, -1]
fold_test_mat <- folded_mat[folded_mat[, 1] == targetFold, -1]
### kNN ###
temp <- knn(fold_train_mat, fold_test_mat, trainSurvived,
k = knn.cv$best.parameters[, 1])
k_NN <- c(k_NN, temp)
### SVM with Linear Kernel ###
linear <- svm(x = fold_train_mat, y = trainSurvived, kernel = 'linear',
cost = linear.svm.cv$best.parameters[1, 1])
temp <- predict(linear, fold_test_mat)
svm.linear <- c(temp, svm.linear)
### SVM with Polynomial Kernel ###
poly <- svm(x = fold_train_mat, y = trainSurvived, kernel = 'polynomial',
cost = poly.svm.cv$best.parameters[1, "cost"],
degree = poly.svm.cv$best.parameters[1, "degree"],
coef0 = poly.svm.cv$best.parameters[1, "coef0"])
temp <- predict(poly, fold_test_mat)
svm.poly <- c(temp, svm.poly)
### SVM with Radial Basis Kernel ###
radial <- svm(x = fold_train_mat, y = trainSurvived, kernel = 'radial',
cost = radial.svm.cv$best.parameters[1, "cost"],
gamma = radial.svm.cv$best.parameters[1, "gamma"],
coef0 = radial.svm.cv$best.parameters[1, "coef0"])
temp <- predict(radial, fold_test_mat)
svm.radial <- c(temp, svm.radial)
### Random Forest ###
set.seed(7)
RF <- ranger(Survived ~ ., data = trainFold, num.trees = 2000)
temp <- predict(RF$forest, testFold)
temp <- temp$predictions
rf <- c(temp, rf)
### Logistic Regression with L1 Regularization ###
logit_L1 <- glmnet(x = fold_train_mat, y = trainSurvived, family = 'binomial',
alpha = 1, lambda = logit_L1.cv$lambda.min)
temp <- predict(logit_L1, fold_test_mat, type = 'response')
temp <- as.vector(ifelse(temp > 0.5, 1, 0))
logitL1 <- c(temp, logitL1)
### Logistic Regression with L2 Regularization ###
logit_L2 <- glmnet(x = fold_train_mat, y = trainSurvived, family = 'binomial',
alpha = 0, lambda = logit_L2.cv$lambda.min)
temp <- predict(logit_L2, fold_test_mat, type = 'response')
temp <- as.vector(ifelse(temp > 0.5, 1, 0))
logitL2 <- c(temp, logitL2)
### XGBoost ###
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = fold_train_mat,
label = as.numeric(trainSurvived) - 1,
objective = "binary:logistic",
verbose = FALSE)
temp <- predict(xgb.titanic, fold_test_mat)
temp <- ifelse(temp > 0.5, 1, 0)
XGB <- c(temp, XGB)
}
meta_train <- cbind(train_mat,
kNN = k_NN - 1,
LinearSVM = svm.linear - 1,
PolySVM = svm.poly - 1,
RadialSVM = svm.radial - 1,
RF = rf - 1,
LogitL1 = logitL1,
LogitL2 = logitL2,
XGB = XGB)
meta_test <- cbind(test_mat,
kNN = as.numeric(knn) - 1,
LinearSVM = as.numeric(linear.svm) - 1,
PolySVM = as.numeric(poly.svm) - 1,
RadialSVM = as.numeric(radial.svm) - 1,
RF = as.numeric(randomForest) - 1,
LogitL1 = logit.L1,
LogitL2 = logit.L2,
XGB = xgb)
doMC::registerDoMC(4)
trCtrl <- trainControl(method = "repeatedcv", number = 5, allowParallel = TRUE)
xgb.grid <- expand.grid(nrounds = seq(50, 120, by = 10),
eta = c(0.01, 0.02, 0.03),
max_depth = 3:7,
gamma = 0,
colsample_bytree = c(0.6, 0.7, 0.8, 0.9, 1),
min_child_weight = c(0.8, 0.9, 1),
subsample = 1
)
set.seed(7)
doMC::registerDoMC(4)
metaXGB <- train(x = meta_train,
y = train$Survived,
objective = "binary:logistic",
trControl = trCtrl,
tuneGrid = xgb.grid,
method = "xgbTree"
)
metaXGB$bestTune
meta.titanic <- xgboost(params = metaXGB$bestTune,
nrounds = metaXGB$bestTune[1, 1],
data = meta_train,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
meta.pred1 <- predict(meta.titanic, meta_test)
meta.pred1 <- ifelse(meta.pred1 > 0.5, 1, 0)
confusionMatrix(meta.pred1, test$Survived, positive = '1')
lambda <- exp(-seq(4, 6, length.out = 400))
set.seed(7)
meta.logit_L1.cv <- cv.glmnet(x = meta_train, y = train$Survived, family = 'binomial',
alpha = 1, lambda = lambda)
meta.pred2 <- predict(meta.logit_L1.cv, meta_test, s = meta.logit_L1.cv$lambda.min, type = 'response')
meta.pred2 <- as.vector(ifelse(meta.pred2 > 0.5, 1, 0))
confusionMatrix(meta.pred2, test$Survived, positive = '1')
meta.logit_L1.cv
lambda <- exp(-seq(5, 6, length.out = 400))
lambda <- exp(-seq(5, 6, length.out = 400))
set.seed(7)
meta.logit_L1.cv <- cv.glmnet(x = meta_train, y = train$Survived, family = 'binomial',
alpha = 1, lambda = lambda)
meta.pred2 <- predict(meta.logit_L1.cv, meta_test, s = meta.logit_L1.cv$lambda.min, type = 'response')
meta.pred2 <- as.vector(ifelse(meta.pred2 > 0.5, 1, 0))
confusionMatrix(meta.pred2, test$Survived, positive = '1')
doMC::registerDoMC(4)
trCtrl <- trainControl(method = "repeatedcv", number = 5, allowParallel = TRUE)
xgb.grid <- expand.grid(nrounds = seq(50, 120, by = 10),
eta = c(0.01, 0.02, 0.03),
max_depth = 3:7,
gamma = c(0, 0.5, 1),
colsample_bytree = c(0.6, 0.7, 0.8, 0.9, 1),
min_child_weight = c(0.8, 0.9, 1),
subsample = 1
)
set.seed(7)
doMC::registerDoMC(4)
metaXGB <- train(x = meta_train,
y = train$Survived,
objective = "binary:logistic",
trControl = trCtrl,
tuneGrid = xgb.grid,
method = "xgbTree"
)
metaXGB$bestTune
meta.titanic <- xgboost(params = metaXGB$bestTune,
nrounds = metaXGB$bestTune[1, 1],
data = meta_train,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
meta.pred1 <- predict(meta.titanic, meta_test)
meta.pred1 <- ifelse(meta.pred1 > 0.5, 1, 0)
confusionMatrix(meta.pred1, test$Survived, positive = '1')
meta.titanic <- xgboost(params = metaXGB$bestTune,
nrounds = metaXGB$bestTune[1, 1],
data = meta_train,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
meta.pred1 <- predict(meta.titanic, meta_test)
meta.pred1 <- ifelse(meta.pred1 > 0.5, 1, 0)
confusionMatrix(meta.pred1, test$Survived, positive = '1')
dim(train())
dim(train)
dim(test)
dim(train_mat)
set.seed(7)
trainIndex <- sample(1:nrow(titanic), 1000)
train <- titanic[trainIndex, ] %>%
select(-c(ID, Name, Ticket))
test <- titanic[-trainIndex, ] %>%
select(-c(ID, Name, Ticket))
train$Survived <- factor(train$Survived)
test$Survived <- factor(test$Survived)
library(Matrix)
train_mat <- model.matrix(data = train, Survived ~ .)[, -1]
test_mat <- model.matrix(data = test, Survived ~ .)[, -1]
dim(train_mat)
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = 210,
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = 200,
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = 200,
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = 180,
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = 210,
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = 220,
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = train_mat,
alpha = 5,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = train_mat,
alpha = 2,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
xgb.titanic <- xgboost(params = xgbTrain$bestTune,
nrounds = xgbTrain$bestTune[1, 1],
data = train_mat,
label = as.numeric(train$Survived) - 1,
objective = "binary:logistic",
verbose = FALSE)
xgb <- predict(xgb.titanic, test_mat)
xgb <- ifelse(xgb > 0.5, 1, 0)
confusionMatrix(xgb, test$Survived, positive = '1')
markdown
markdown("2016-12-20-adaptive-lasso", "machine_learning")
markdown("2016-12-20-adaptive-lasso.Rmd", "machine_learning")
