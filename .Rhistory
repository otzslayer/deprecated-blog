numNode <- length(V(network))
sumWeight <- round(sum(E(network)$weight), 2)
averageWeight <- round(sumWeight/numNode, 2)
cat("#Edge:", numEdge, "\n#Node:", numNode, "\nTotal Transfer Fee:", paste0(sumWeight, "m\nAverage Transfer Fee per Event: ", paste0(averageWeight, "m\n")))
}
networkInfo(epl.network)
networkInfo(laliga.network)
networkInfo(bundes.network)
networkInfo(seriea.network)
write_csv(team_links, "~/Desktop/team_links.csv")
write_csv(team_nodes, "~/Desktop/team_nodes.csv")
library(RJSONIO)
library(dplyr)
library(stringr)
library(readr)
json <- read_lines("~/Google Drive/log.txt")
head(json, 40)
index <- !(1:length(json) %% 15 == 1 | 1:length(json) %% 15 == 4 |  1:length(json) %% 15 == 5 | 1:length(json) %% 15 == 6 | 1:length(json) %% 15 == 7 | 1:length(json) %% 15 == 8 | 1:length(json) %% 15 == 12)
json <- json[index]
head(json, 40)
clean_json <- json %>%
str_replace("ObjectId\\(", "") %>%
str_replace("ISODate\\(", "") %>%
str_replace("\\)", "") %>%
str_replace("    ", "") %>%
str_replace(" :", ":") %>%
str_replace("\\}", "\\}\n")
head(clean_json, 40)
write(clean_json, "~/Google Drive/clean_log.json")
test <- paste(clean_json, collapse = " ")
write(test, "~/Google Drive/test.txt")
going_data <- jsonlite::stream_in(file("~/Google Drive/test.txt"))
head(going_data)
library(readr)
write_csv(going_data, "~/Desktop/Baek.csv")
credit <- read.csv("~/Google Drive/ML Lecture/8 Tree Based/credit.csv")
summary(credit)
# install.packages(c("rpart", "rpart.plot"))
library(caret)
library(rpart)
library(rpart.plot)
# CART Algorithm
set.seed(123)
trainIdx <- sample(1:nrow(credit), size = nrow(credit) * 0.7)
trainCredit <- credit[trainIdx, ]
testCredit <- credit[-trainIdx, ]
creditTree <- rpart(default ~ ., data = trainCredit, method = "class")
creditTree
rpart.plot(creditTree)
rpart.plot(creditTree)
?rpart.plot
predictCredit <- predict(creditTree, testCredit, type = "class")
predictCredit <- predict(creditTree, testCredit, type = "class")
confusionMatrix(predictCredit, testCredit$default)
install.packages(c("randomForest", "ranger"))
library(randomForest)
set.seed(1234)
RF_Credit <- randomForest(default ~ ., data = trainCredit, importance = TRUE, replace = TRUE, proximity = TRUE, mtry = 4)
RF_Credit
varImpPlot(RF_Credit)
pred_RF_Credit <- predict(RF_Credit, testCredit)
pred_RF_Credit <- predict(RF_Credit, testCredit)
confusionMatrix(pred_RF_Credit, testCredit$default)
head(pred_RF_Credit)
confusionMatrix(pred_RF_Credit, testCredit$default)
library(ranger)
set.seed(1234)
RF_Credit2 <- ranger(default ~ ., data = trainCredit,
num.trees = 2000, importance = "impurity", replace = TRUE, write.forest = TRUE)
RF_Credit2
pred_RF_Credit <- predict(RF_Credit2, testCredit)
pred_RF_Credit
pred_RF_Credit$predictions
confusionMatrix(pred_RF_Credit$predictions, testCredit$default)
# install.packages("xgboost")
library(xgboost)
doMC::registerDoMC(4)
str(trainCredit)
trainLabel <- as.numeric(trainCredit$default) - 1
testLabel <- as.numeric(testCredit$default) - 1
trainMat <- model.matrix(default ~ ., data = trainCredit)[, -1]
testMat <- model.matrix(default ~ ., data = testCredit)[, -1]
View(trainMat)
trainMat <- model.matrix(default ~ ., data = trainCredit)[, -1]
testMat <- model.matrix(default ~ ., data = testCredit)[, -1]
credit_xgboost <- xgboost(data = trainMat,
label = trainLabel,
max.depth = 6,
eta = 0.3,
subsample = 1,
nrounds = 10,
objective = "binary:logistic",
eval_metric = "error"
)
xgb_pred <- predict(credit_xgboost, testMat)
head(xgb_pred)
xgb_pred <- ifelse(xgb_pred > 0.5, 1, 0)
confusionMatrix(xgb_pred, testCredit$default)
confusionMatrix(xgb_pred, testLabel)
set.seed(1234)
credit_xgboost <- xgboost(data = trainMat,
label = trainLabel,
max.depth = 6,
eta = 0.3,
subsample = 1,
nrounds = 10,
objective = "binary:logistic",
eval_metric = "error"
)
xgb_pred <- predict(credit_xgboost, testMat)
xgb_pred <- ifelse(xgb_pred > 0.5, 1, 0)
confusionMatrix(xgb_pred, testLabel)
library(caret)
cv.ctrl <- trainControl(method = "repeatedcv", number = 5, allowParallel = TRUE)
xgb.grid <- expand.grid(nrounds = seq(10, 30, by = 5),
eta = c(0.1, 0.3, 0.5),
max_depth = c(3, 5, 7),
gamma = 0,
colsample_bytree = c(0.8, 0.9, 1),
min_child_weight = c(0.8, 0.9, 1),
subsample = 1
)
head(xgb.grid)
set.seed(1234)
xgb_tune <- train(trainMat, factor(trainLabel),
method = "xgbTree", trControl = cv.ctrl,
tuneGrid = xgb.grid, verbose = TRUE,
metric = "error", nthread = 4)
xgb_tune$bestTune
xgb_tune$bestTune
credit_xgboost2 <- xgboost(data = trainMat,
label = trainLabel,
nrounds = 20,
max_depth = 3,
eta = 0.5,
gamma = 0,
colsample_bytree = 0.8,
min_child_weight = 0.8,
subsample = 1,
objective = "binary:logistic",
eval_metric = "error")
xgb_pred2 <- predict(credit_xgboost2, testMat)
xgb_pred2 <- ifelse(xgb_pred2 > 0.5, 1, 0)
confusionMatrix(xgb_pred2, testLabel)
confusionMatrix(xgb_pred, testLabel)
xgb_tune$bestTune
Sys.Date()
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
setwd("/Users/Han/Google Drive/ML Lecture/MLPB-master/Problems/Classify Dart Throwers")
library(data.table)
library(ggplot2)
#======================================================================================================
# Helper method for generating a random ellipse
rEllipse <- function(n=1, width=1, height=1, center=c(x=0, y=0)){
# Generate random points inside an ellipse
rho <- runif(n=n)
phi <- runif(n=n, min=0, max=2*pi)
x <- (sqrt(rho) * cos(phi)) * width / 2 + center[1]
y <- (sqrt(rho) * sin(phi)) * height / 2 + center[2]
result <- data.table(x=x, y=y)
return(result)
}
#======================================================================================================
# Make data
set.seed(1)
bob <- rbind(
rEllipse(5, width=2, height=2, center=c(0, 0)),
rEllipse(210, width=2, height=2, center=c(0, 0))[x^2 + y^2 > .9^2]
)
sue <- rbind(
rEllipse(5, width=2, height=2, center=c(0, 0)),
rEllipse(210, width=2, height=2, center=c(0, 0))[between(x^2 + y^2, .7^2, .9^2)]
)[sample(.N, 50)]
mark <- rbind(
rEllipse(10, width=2, height=2, center=c(0, 0)),
rEllipse(1000, width=2, height=2, center=c(0, 0))[between(x, -.275, .275) & between(y, -.6, .6)]
)[sample(.N, 50)]
kate <- rbind(
rEllipse(10, width=2, height=2, center=c(0, 0)),
rEllipse(3, width=.2, height=.1, center=c(.3, -.6)),
rEllipse(4, width=.2, height=.1, center=c(.3, -.3)),
rEllipse(3, width=.2, height=.1, center=c(.3, 0)),
rEllipse(4, width=.2, height=.1, center=c(.3, .3)),
rEllipse(3, width=.2, height=.1, center=c(.3, .6)),
rEllipse(4, width=.2, height=.1, center=c(-.3, -.6)),
rEllipse(3, width=.2, height=.1, center=c(-.3, -.3)),
rEllipse(4, width=.2, height=.1, center=c(-.3, 0)),
rEllipse(3, width=.2, height=.1, center=c(-.3, .3)),
rEllipse(4, width=.2, height=.1, center=c(-.3, .6))
)
#--------------------------------------------------
# Combine competitor throws
dt <- rbind(
bob[, list(x, y, Competitor="Bob")],
sue[, list(x, y, Competitor="Sue")],
mark[, list(x, y, Competitor="Mark")],
kate[, list(x, y, Competitor="Kate")]
)
# Check distribution
dt[, .N, keyby=Competitor]
# Adjust column names
setnames(dt, c("x", "y"), c("XCoord", "YCoord"))
# Ranom Shuffle
dt <- dt[sample(.N, .N)]
# Make Competitor a factor
dt[, Competitor := factor(Competitor)]
# Add ID column
dt[, ID := .I]
# Fix column order
setcolorder(dt, c("ID", "XCoord", "YCoord", "Competitor"))
# Plot
ggplot(dt, aes(x=XCoord, y=YCoord, color=Competitor))+geom_point()
#======================================================================================================
# Partition data
#--------------------------------------------------
# Split into train and test
train <- dt[sample(.N, round(nrow(dt) * .8))]
test <- dt[!train, on="ID"]
#======================================================================================================
# Write Files to disc
# fwrite(train, "/Users/Ben/Businesses/GormAnalysis/Internal Projects/MLPB/Problems/Classify Dart Throwers/_Data/train.csv")
# fwrite(test, "/Users/Ben/Businesses/GormAnalysis/Internal Projects/MLPB/Problems/Classify Dart Throwers/_Data/test.csv")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
library(knitr)
library(rmdformats)
library(ggplot2)
library(dplyr)
library(ggthemr)
library(printr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, prompt = FALSE,
tidy = FALSE, comment = NA, warning = FALSE,
fig.height = 5, fig.width = 8, fig.retina = 2,
fig.align = "center")
custom_theme <- theme_bw(base_family = "Open Sans") +
theme(axis.title.x = element_text(size = 11,
margin = margin(10, 0, 0, 0),
face = "bold"),
axis.title.y = element_text(size = 11,
margin = margin(0, 10, 0, 0),
face = "bold"),
plot.title = element_text(family = "Open Sans", face = "bold", size = 20),
panel.background = element_blank(),
axis.text.x = element_text(angle = 0, face = "italic", vjust = .1),
axis.text.y = element_text(face = "italic"), legend.position = "bottom",
legend.title = element_text(size = 9, face = 'bold.italic'))
theme_set(custom_theme)
library(data.table)
library(data.table)
library(ggplot2)
rEllipse <- function(n=1, width=1, height=1, center=c(x=0, y=0)){
# Generate random points inside an ellipse
rho <- runif(n=n)
phi <- runif(n=n, min=0, max=2*pi)
x <- (sqrt(rho) * cos(phi)) * width / 2 + center[1]
y <- (sqrt(rho) * sin(phi)) * height / 2 + center[2]
result <- data.table(x=x, y=y)
return(result)
}
set.seed(1)
bob <- rbind(
rEllipse(5, width=2, height=2, center=c(0, 0)),
rEllipse(210, width=2, height=2, center=c(0, 0))[x^2 + y^2 > .9^2]
)
sue <- rbind(
rEllipse(5, width=2, height=2, center=c(0, 0)),
rEllipse(210, width=2, height=2, center=c(0, 0))[between(x^2 + y^2, .7^2, .9^2)]
)[sample(.N, 50)]
mark <- rbind(
rEllipse(10, width=2, height=2, center=c(0, 0)),
rEllipse(1000, width=2, height=2, center=c(0, 0))[between(x, -.275, .275) & between(y, -.6, .6)]
)[sample(.N, 50)]
kate <- rbind(
rEllipse(10, width=2, height=2, center=c(0, 0)),
rEllipse(3, width=.2, height=.1, center=c(.3, -.6)),
rEllipse(4, width=.2, height=.1, center=c(.3, -.3)),
rEllipse(3, width=.2, height=.1, center=c(.3, 0)),
rEllipse(4, width=.2, height=.1, center=c(.3, .3)),
rEllipse(3, width=.2, height=.1, center=c(.3, .6)),
rEllipse(4, width=.2, height=.1, center=c(-.3, -.6)),
rEllipse(3, width=.2, height=.1, center=c(-.3, -.3)),
rEllipse(4, width=.2, height=.1, center=c(-.3, 0)),
rEllipse(3, width=.2, height=.1, center=c(-.3, .3)),
rEllipse(4, width=.2, height=.1, center=c(-.3, .6))
)
dt <- rbind(
bob[, list(x, y, Competitor="Bob")],
sue[, list(x, y, Competitor="Sue")],
mark[, list(x, y, Competitor="Mark")],
kate[, list(x, y, Competitor="Kate")]
)
setnames(dt, c("x", "y"), c("XCoord", "YCoord"))
dt <- dt[sample(.N, .N)]
dt[, Competitor := factor(Competitor)]
dt[, ID := .I]
setcolorder(dt, c("ID", "XCoord", "YCoord", "Competitor"))
ggplot(dt, aes(x=XCoord, y=YCoord, color=Competitor))+geom_point()
train <- dt[sample(.N, round(nrow(dt) * .8))]
test <- dt[!train, on="ID"]
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
ggplot(dt, aes(x=XCoord, y=YCoord, color=Competitor))+geom_point() + coord_equal()
library(gridExtra)
gridExtra::grid.arrange(g1, g2)
g1 <- ggplot(train, aes(x=XCoord, y=YCoord, color=Competitor)) +
geom_point() +
coord_equal() +
ggtitle("Training Darts")
g2 <- ggplot(test, aes(x=XCoord, y=YCoord)) +
geom_point() +
coord_equal() +
ggtitle("Test Darts")
gridExtra::grid.arrange(g1, g2)
gridExtra::grid.arrange(g1, g2, nrow = 1)
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
options(scipen=10)
library(data.table)
library(ggplot2)
library(mltools)  # For generating CV folds and one-hot-encoding
library(class)  # K-Nearest Neighbors model
library(LiblineaR)  # Support Vector Machine and Logistic Regression
train <- fread("_Data/train.csv")
setwd("/Users/Han/Google Drive/ML Lecture/MLPB-master/Problems/Classify Dart Throwers")
train <- fread("_Data/train.csv")
test <- fread("_Data/test.csv")
train[, Competitor := factor(Competitor)]
test[, Competitor := factor(Competitor)]
train[, DistFromCenter := sqrt(XCoord^2 + YCoord^2)]
test[, DistFromCenter := sqrt(XCoord^2 + YCoord^2)]
train[, FoldID := folds(Competitor, nfolds=5, stratified=TRUE, seed=2016)]  # mltools function
library(mltools)  # For generating CV folds and one-hot-encoding
install.packages("mltools")
library(mltools)  # For generating CV folds and one-hot-encoding
library(class)  # K-Nearest Neighbors model
library(LiblineaR)  # Support Vector Machine and Logistic Regression
install.packages("LiblineaR")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
train <- fread("_Data/train.csv")
test <- fread("_Data/test.csv")
train[, Competitor := factor(Competitor)]
train[, Competitor := factor(Competitor)]
test[, Competitor := factor(Competitor)]
train[, DistFromCenter := sqrt(XCoord^2 + YCoord^2)]
test[, DistFromCenter := sqrt(XCoord^2 + YCoord^2)]
train[, FoldID := folds(Competitor, nfolds=5, stratified=TRUE, seed=2016)]  # mltools function
knnCV <- list()
knnCV[["Features"]] <- c("XCoord", "YCoord")
knnCV[["ParamGrid"]] <- CJ(k=seq(1, 30))
knnCV[["BestScore"]] <- 0
for(i in seq_len(nrow(knnCV[["ParamGrid"]]))){
# Get the ith set of parameters
params <- knnCV[["ParamGrid"]][i]
# Build an empty vector to store scores from each train/test fold
scores <- numeric()
# Build an empty list to store predictions from each train/test fold
predsList <- list()
# Loop through each test fold, fit model to training folds and make predictions on test fold
for(foldID in 1:5){
# Build the train/test folds
testFold <- train[J(FoldID=foldID), on="FoldID"]
trainFolds <- train[!J(FoldID=foldID), on="FoldID"]  # Exclude fold i from trainFolds
# Train the model & make predictions
testFold[, Pred := knn(train=trainFolds[, knnCV$Features, with=FALSE], test=testFold[, knnCV$Features, with=FALSE], cl=trainFolds$Competitor, k=params$k)]
predsList <- c(predsList, list(testFold[, list(ID, FoldID, Pred)]))
# Evaluate predictions (accuracy rate) and append score to scores vector
score <- mean(testFold$Pred == testFold$Competitor)
scores <- c(scores, score)
}
# Measure the overall score. If best, tell knnCV
score <- mean(scores)
# Insert the score into ParamGrid
knnCV[["ParamGrid"]][i, Score := score][]
print(paste("Params:", paste(colnames(knnCV[["ParamGrid"]][i]), knnCV[["ParamGrid"]][i], collapse = " | ")))
if(score > knnCV[["BestScore"]]){
knnCV[["BestScores"]] <- scores
knnCV[["BestScore"]] <- score
knnCV[["BestParams"]] <- knnCV[["ParamGrid"]][i]
knnCV[["BestPreds"]] <- rbindlist(predsList)
}
}
knnCV[["BestParams"]]
knnCV[["ParamGrid"]]
ggplot(knnCV[["ParamGrid"]], aes(x=k, y=Score))+geom_line()+geom_point()
knnCV
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
?scale_color_discrete
?scale_x_discrete
library(knitr)
library(rmdformats)
library(ggplot2)
library(dplyr)
library(ggthemr)
library(printr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, prompt = FALSE,
tidy = FALSE, comment = NA, warning = FALSE, cache = TRUE,
fig.height = 5, fig.width = 8, fig.retina = 2,
fig.align = "center")
custom_theme <- theme_bw(base_family = "Open Sans") +
theme(axis.title.x = element_text(size = 11,
margin = margin(10, 0, 0, 0),
face = "bold"),
axis.title.y = element_text(size = 11,
margin = margin(0, 10, 0, 0),
face = "bold"),
plot.title = element_text(family = "Open Sans", face = "bold", size = 20),
panel.background = element_blank(),
axis.text.x = element_text(angle = 0, face = "italic", vjust = .1),
axis.text.y = element_text(face = "italic"), legend.position = "bottom",
legend.title = element_text(size = 9, face = 'bold.italic'))
theme_set(custom_theme)
ggplot(svmCV[["ParamGrid"]], aes(x=cost, y=Score, color=factor(type))) +
geom_line() +
geom_point() +
scale_x_discrete(labels = 1:5)
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
metas.knn <- knnCV[["BestPreds"]]
metas.svm <- svmCV[["BestPreds"]]
svmCV <- list()
svmCV[["Features"]] <- c("XCoord", "YCoord", "DistFromCenter")
svmCV[["ParamGrid"]] <- CJ(type=1:5, cost=c(.01, .1, 1, 10, 100, 1000, 2000), Score=NA_real_)
svmCV[["BestScore"]] <- 0
# Loop through each set of parameters
for(i in seq_len(nrow(svmCV[["ParamGrid"]]))){
# Get the ith set of parameters
params <- svmCV[["ParamGrid"]][i]
# Build an empty vector to store scores from each train/test fold
scores <- numeric()
# Build an empty list to store predictions from each train/test fold
predsList <- list()
# Loop through each test fold, fit model to training folds and make predictions on test fold
for(foldID in 1:5){
# Build the train/test folds
testFold <- train[J(FoldID=foldID), on="FoldID"]
trainFolds <- train[!J(FoldID=foldID), on="FoldID"]  # Exclude fold i from trainFolds
# Train the model & make predictions
svm <- LiblineaR(data=trainFolds[, svmCV$Features, with=FALSE], target=trainFolds$Competitor, type=params$type, cost=params$cost)
testFold[, Pred := predict(svm, testFold[, svmCV$Features, with=FALSE])$predictions]
predsList <- c(predsList, list(testFold[, list(ID, FoldID, Pred)]))
# Evaluate predictions (accuracy rate) and append score to scores vector
score <- mean(testFold$Pred == testFold$Competitor)
scores <- c(scores, score)
}
# Measure the overall score. If best, tell svmCV
score <- mean(scores)
# Insert the score into ParamGrid
svmCV[["ParamGrid"]][i, Score := score][]
print(paste("Params:", paste(colnames(svmCV[["ParamGrid"]][i]), svmCV[["ParamGrid"]][i], collapse = " | ")))
if(score > svmCV[["BestScore"]]){
svmCV[["BestScores"]] <- scores
svmCV[["BestScore"]] <- score
svmCV[["BestParams"]] <- svmCV[["ParamGrid"]][i]
svmCV[["BestPreds"]] <- rbindlist(predsList)
}
}
# Check the best parameters
svmCV[["BestParams"]]
# Plot the score for each (cost, type) pairs
svmCV[["ParamGrid"]]
ggplot(svmCV[["ParamGrid"]], aes(x=cost, y=Score, color=factor(type)))+geom_line()+geom_point()
metas.knn <- knnCV[["BestPreds"]]
metas.svm <- svmCV[["BestPreds"]]
metas.knn
data.frame(metas.knn)
kNNCV
knnCV
train[metas.knn, Meta.knn := Pred, on="ID"]
train[metas.svm, Meta.svm := Pred, on="ID"]
train
test[, Meta.knn := knn(train=train[, knnCV$Features, with=FALSE], test=test[, knnCV$Features, with=FALSE], cl=train$Competitor, k=knnCV$BestParams$k)]
svm <- LiblineaR(data=train[, svmCV$Features, with=FALSE], target=train$Competitor, type=svmCV$BestParams$type, cost=svmCV$BestParams$cost)
test[, Meta.svm := predict(svm, test[, svmCV$Features, with=FALSE])$predictions]
metas.knn <- knnCV[["BestPreds"]]
metas.svm <- svmCV[["BestPreds"]]
train[metas.knn, Meta.knn := Pred, on="ID"]
train[metas.svm, Meta.svm := Pred, on="ID"]
train <- one_hot(train, cols=c("Meta.knn", "Meta.svm"), dropCols=FALSE)
train
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
markdown("2016-12-28-model-stacking.Rmd", "machine_learning")
